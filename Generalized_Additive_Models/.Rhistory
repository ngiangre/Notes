lines(s,Xp %*% coef(b))
prs.fit <- function(y,x,xj,sp){
X = tf.X(x,xj) ## Model Matrix
D <- diff(diag(length(xj)),differences=2) ## sqrt penalty
X <- rbind(X,sqrt(sp) * D) ## augmented model matrix
y <- c(y,rep(0,nrow(D))) # augmented data
lm(y ~ X - 1) ## penalized least squares fit
}
sj <- seq(min(size),max(size),length=20) ## knots
b <- prs.fit(wear,size,sj,2) ## penalized fit
plot(size,wear)
Xp <- tf.X(s,sj) ## prediction matrix
lines(s,Xp %*% coef(b))
rho = seq(-9,11,length=90)
n <- length(wear)
V <- rep(NA,90)
for(i in 1:90){ ## loop through smoothing params
b <- prs.fit(wear,size,sj,exp(rho[i])) ## fit model
trF <- sum(influence(b)$hat[1:n]) ## extract EDF
rss <- sum((wear - fitted(b)[1:n])^2) ## residual SS
V[i] <- n*rss/(n-trF)^2 ## GCV score
}
plot(rho,V,type="l",xlab=expression(log(lambda)),main="GCV score")
sp <- exp(rho[V==min(V)]) ## extract optimal sp
b <- prs.fit(wear,size,sj,sp) ## re-fit
plot(size,wear,main="GCV optimal fit")
lines(s, Xp %*% coef(b))
## copy of llm from 2.2.4...
llm <- function(theta,X,Z,y) {
## untransform parameters...
sigma.b <- exp(theta[1])
sigma <- exp(theta[2])
## extract dimensions...
n <- length(y); pr <- ncol(Z); pf <- ncol(X)
## obtain \hat \beta, \hat b...
X1 <- cbind(X,Z)
ipsi <- c(rep(0,pf),rep(1/sigma.b^2,pr))
b1 <- solve(crossprod(X1)/sigma^2+diag(ipsi),
t(X1)%*%y/sigma^2)
## compute log|Z'Z/sigma^2 + I/sigma.b^2|...
ldet <- sum(log(diag(chol(crossprod(Z)/sigma^2 +
diag(ipsi[-(1:pf)])))))
## compute log profile likelihood...
l <- (-sum((y-X1%*%b1)^2)/sigma^2 - sum(b1^2*ipsi) -
n*log(sigma^2) - pr*log(sigma.b^2) - 2*ldet - n*log(2*pi))/2
attr(l,"b") <- as.numeric(b1) ## return \hat beta and \hat b
-l
}
X0 <- tf.X(size,sj) ## X in original parameterization
D <- rbind(0,0,diff(diag(20),difference=2))
diag(D) <- 1 ## augmented D
X <- t(backsolve(t(D),t(X0))) ## re-parameterized X
Z <- X[,-c(1,2)]; X <- X[,1:2] ## mixed model matrices
## estimate smoothing and variance parameters
m <- optim(c(0,0),llm,method="BFGS",X=X,Z=Z,y=wear)
b <- attr(llm(m$par,X,Z,wear),"b") ## extract coefficients
## plot results
plot(size,wear)
Xp1 <- t(backsolve(t(D),t(Xp))) ## re-parameterized pred. mat.
lines(s,Xp1 %*% as.numeric(b),col="grey",lwd=2)
library(nlme)
g <- factor(rep(1,nrow(X))) ## dummy factor
m <- lme(wear ~ X - 1, random=list(g=pdIdent(~ Z-1)))
lines(s,Xp1  %*% as.numeric(coef(m))) ## add to plot
# Additive models ---------------------------------------------------------
#' When two smooth functions are in an equation there coould be an identifiability problem because
#' each function is estimable to within an additive constant - applying any constant to the functions
#' do not change the model predictions
#'
#' But if that's addressed, you can used penalized regression splines, estimate with penalized LSs and select the degree of smoothing by cross validation of REML for the model
#'
#' calls the function producing the unconstrained basis and square root penalty matrices, given knot sequence xk and covariate values x
tf.XD <- function(x,xk,cmx=NULL,m=2){
## get X and D subject to constraint
nk <- length(xk)
X <- tf.X(x,xk)[,-nk] ## basis matrix
D <- diff(diag(nk),differences=m)[,-nk] ## root penalty
if(is.null(cmx)) cmx <- colMeans(X)
X <- sweep(X,2,cmx) ## subtract cmx from columns
list(X=X,D=D,cmx=cmx)
}
am.fit <- function(y,x,v,sp,k=10){
## setup bases and penalties...
xk <- seq(min(x),max(x),length=k)
xdx <- tf.XD(x,xk)
vk <- seq(min(v),max(v),length=k)
xdv <- tf.XD(v,vk)
## create augmented model matrix and response...
nD <- nrow(xdx$D)*2
sp <- sqrt(sp)
X <- cbind(c(rep(1,nrow(xdx$X)),rep(0,nD)),
rbind(xdx$X,sp[1]*xdx$D,xdx$D*0),
rbind(xdv$X,xdx$D*0,sp[2]*xdv$D))
y1 <- c(y,rep(0,nD))
## fit model...
b <- lm(y1 ~ X - 1)
## compute some useful quuantities...
n <- length(y)
trA <- sum(influence(b)$hat[1:n]) ## EDF
rsd <- y - fitted(b)[1:n] ## residuals
rss <- sum(rsd^2) ## residual SS
sig.hat <- rss/(n-trA) ## residual variance
gcv <- sig.hat*n/(n-trA) ## GCV score
Vb <- vcov(b)*sig.hat/summary(b)$sigma^2 ## coef cov matrix
## return fittted model...
list(b=coef(b),Vb=Vb,edf=trA,gcv=gcv,fitted=fitted(b)[1:n],
rsd=rsd,xk=list(xk,vk),cmx=list(xdx$cmx,xdv$cmx))
}
am.gcv <- function(lsp,y,x,v,k){
## function suitable for GCV optimization by optim
am.fit(y,x,v,exp(lsp),k)$gcv
}
## find GCV optimal smoothing parameters...
fit <- optim(c(0,0),am.gcv,y=trees$Volume,x=trees$Girth,
v=trees$Height,k=10)
sp <- exp(fit$par) ## best fit smoothing parameters
## Get fit at GCV optimal smoothing parameters
fit <- am.fit(trees$Volume,trees$Girth,trees$Height,sp,k=10)
am.plot <- function(fit,xlab,ylab){
## prodduces effect plots for simple 2 term
## additive model
start <- 2 ## where smooth coeffs start in beta
for(i in 1:2){
## sequence of values at which to predict...
x <- seq(min(fit$xk[[i]]),max(fit$xk[[i]]),length=200)
## get prediction matrix for this smooth...
Xp <- tf.XD(x,fit$xk[[i]],fit$cmx[[i]])$X
## extract coefficients and cov matrix for this smooth
stop <- start + ncol(Xp)-1; ind <- start:stop
b <- fit$b[ind];Vb <- fit$Vb[ind,ind]
## values for smooth at x...
fv <- Xp %*% b
## standard errors for smooth at x...
se <- rowSums((Xp %*% Vb)* Xp)^.5
## 2 se limits for smooth
ul <- fv + 2 * se; ll <- fv - 2 * se
## plot smooth and limits
plot(x,fv,type="l",ylim=range(c(ul,ll)),xlab=xlab[i],ylab=ylab[i])
lines(x,ul,lty=2); lines(x,ll,lty=2)
start <- stop+1
}
}
par(mfrow=c(1,3))
plot(fit$fitted,trees$Volume,xlab="fitted volume",ylab="observed volume")
am.plot(fit, xlab=c("Girth","Height"),
ylab=c("s(Girth)","s(Height)"))
#' the zero width point for height in the interval occurs because sum to zero constraint exactly  determines where the straight line must pass through zero
#'
# Generalized additive models ---------------------------------------------
#' The linear predictor now predicts some known smooth monotonic function of the expected value of the response, and the response may follow any exponential family disrttibution.
#' The model is estimated by penalized maximum likelihood versus penalized least squares but in practice it is penalized iterative least squares (PIRLS)
#'
gam.fit <- function(y,x,v,sp,k=10){
## gamma error log link 2 term gam fit...
eta <- log(y) ## initial eta
not.converged <- TRUE
old.gcv <- -100 ## don't converge immediately
while(not.converged){
mu <- exp(eta) ## current mu estimate
z <- (y - mu)/mu+eta ## pseudodata
fit <- am.fit(z,x,v,sp,k) ## penalized least squares
if(abs(fit$gcv-old.gcv)<1e-5*fit$gcv){
not.converged <- FALSE
}
old.gcv <- fit$gcv
eta <- fit$fitted ## updated eta
}
fit$fittted <- exp(fit$fitted) ## mue
fit
}
gam.gcv <- function(lsp,y,x,v,k=10){
gam.fit(y,x,v,exp(lsp),k=k)$gcv
}
## find GCV optimal smoothing parameters...
fit <- optim(c(0,0),gam.gcv,y=trees$Volume,x=trees$Girth,
v=trees$Height,k=10)
sp <- exp(fit$par) ## best fit smoothing parameters
## Get fit at GCV optimal smoothing parameters
fit <- gam.fit(trees$Volume,trees$Girth,trees$Height,sp,k=10)
par(mfrow=c(1,3))
plot(fit$fitted,trees$Volume,xlab="fitted volume",ylab="observed volume")
am.plot(fit, xlab=c("Girth","Height"),
ylab=c("s(Girth)","s(Height)"))
# Summary ------------------------------------------------------------------
#' Estimation is by penalized versions of the least squares and maximum-likelihood methods used for linear models and GLMs.
#' Indeed technically GAMs are simply GLMs estimated subject to smoothing penalties
#' Largest difficulty is estimating the degree of penalization - GCV gives a reasonable solution but so does marginal likelihood
#'
# Introducing package mgcv ------------------------------------------------
library(mgcv)
data(trees)
ct1 <- gam(Volume ~ s(Height) + s(Girth),
family=Gamma(link=log),data=trees)
ct1
plot(ct1,residuals=T)
source('~/GitHub/Notes/Generalized_Additive_Models/Chapter4_Introducing_GAMs.R')
# Inroduction -------------------------------------------------------------
# Univariate Smoothing ----------------------------------------------------
#' Starting from y=mx+b => y=f(x)+b where
#' y is the response, x is a covariate, f is a smooth function, and b is the error
#'
#' f(x)=sum(b_j * Beta) (basis function where f is an element of a space of functions)
#'
#' so for one covariate, y=b_j * Beta + b
#'
#' or f could be a 4th order polynomial and so
#' y=beta0 + x*Beta1 + x^2 * Beta2 + x^3 * Beta3 + x^4 * Beta4 + b
#'
#' But polynomial bases are problematic in that taylor's theorem does well when estimating around a single point but not around it's entire domain --> the polynomial oscillates wildly in places in order to interpolate and to sustain the derivative continuities at the points. A piecewise linear function does much better - doesn't oscillate wildly. Splines are shown to be a further improvement from this.
#'
require(gamair)
data("engine");attach(engine)
plot(size,wear,xlab="Engine capacity",ylab="Wear index")
tf <- function(x,xj,j){
## generate the jth tent function from set defined by knots xj
dj <- xj*0;dj[j] <- 1
## linearly interpolates between xj and dj taking place at x
approx(xj,dj,x)$y
}
tf.X <- function(x,xj){
## tent function basis matrix given data X
## and knot sequence xk
nk <- length(xj); n <- length(x)
X <- matrix(NA,n,nk)
for(j in 1:nk) X[,j] <- tf(x,xj,j)
X
}
sj <- seq(min(size), max(size),length=6)
X <- tf.X(size,sj) # sj determines the number of columns because it is the knots and size is the base vector where to extrapolate from - for(i in 1:length(sj))plot(x,tf(x,xj,i)) - at each knot value Xij is 1 and interpolates to 0 from either side
b <- lm(wear ~ X - 1)
s <- seq(min(size),max(size),length=200)
Xp <- tf.X(s,sj)
plot(size,wear)
lines(s,Xp %*% coef(b))
prs.fit <- function(y,x,xj,sp){
X = tf.X(x,xj) ## Model Matrix
D <- diff(diag(length(xj)),differences=2) ## sqrt penalty
X <- rbind(X,sqrt(sp) * D) ## augmented model matrix
y <- c(y,rep(0,nrow(D))) # augmented data
lm(y ~ X - 1) ## penalized least squares fit
}
sj <- seq(min(size),max(size),length=20) ## knots
b <- prs.fit(wear,size,sj,2) ## penalized fit
plot(size,wear)
Xp <- tf.X(s,sj) ## prediction matrix
lines(s,Xp %*% coef(b))
rho = seq(-9,11,length=90)
n <- length(wear)
V <- rep(NA,90)
for(i in 1:90){ ## loop through smoothing params
b <- prs.fit(wear,size,sj,exp(rho[i])) ## fit model
trF <- sum(influence(b)$hat[1:n]) ## extract EDF
rss <- sum((wear - fitted(b)[1:n])^2) ## residual SS
V[i] <- n*rss/(n-trF)^2 ## GCV score
}
plot(rho,V,type="l",xlab=expression(log(lambda)),main="GCV score")
sp <- exp(rho[V==min(V)]) ## extract optimal sp
b <- prs.fit(wear,size,sj,sp) ## re-fit
plot(size,wear,main="GCV optimal fit")
lines(s, Xp %*% coef(b))
## copy of llm from 2.2.4...
llm <- function(theta,X,Z,y) {
## untransform parameters...
sigma.b <- exp(theta[1])
sigma <- exp(theta[2])
## extract dimensions...
n <- length(y); pr <- ncol(Z); pf <- ncol(X)
## obtain \hat \beta, \hat b...
X1 <- cbind(X,Z)
ipsi <- c(rep(0,pf),rep(1/sigma.b^2,pr))
b1 <- solve(crossprod(X1)/sigma^2+diag(ipsi),
t(X1)%*%y/sigma^2)
## compute log|Z'Z/sigma^2 + I/sigma.b^2|...
ldet <- sum(log(diag(chol(crossprod(Z)/sigma^2 +
diag(ipsi[-(1:pf)])))))
## compute log profile likelihood...
l <- (-sum((y-X1%*%b1)^2)/sigma^2 - sum(b1^2*ipsi) -
n*log(sigma^2) - pr*log(sigma.b^2) - 2*ldet - n*log(2*pi))/2
attr(l,"b") <- as.numeric(b1) ## return \hat beta and \hat b
-l
}
X0 <- tf.X(size,sj) ## X in original parameterization
D <- rbind(0,0,diff(diag(20),difference=2))
diag(D) <- 1 ## augmented D
X <- t(backsolve(t(D),t(X0))) ## re-parameterized X
Z <- X[,-c(1,2)]; X <- X[,1:2] ## mixed model matrices
## estimate smoothing and variance parameters
m <- optim(c(0,0),llm,method="BFGS",X=X,Z=Z,y=wear)
b <- attr(llm(m$par,X,Z,wear),"b") ## extract coefficients
## plot results
plot(size,wear)
Xp1 <- t(backsolve(t(D),t(Xp))) ## re-parameterized pred. mat.
lines(s,Xp1 %*% as.numeric(b),col="grey",lwd=2)
library(nlme)
g <- factor(rep(1,nrow(X))) ## dummy factor
m <- lme(wear ~ X - 1, random=list(g=pdIdent(~ Z-1)))
lines(s,Xp1  %*% as.numeric(coef(m))) ## add to plot
# Additive models ---------------------------------------------------------
#' When two smooth functions are in an equation there coould be an identifiability problem because
#' each function is estimable to within an additive constant - applying any constant to the functions
#' do not change the model predictions
#'
#' But if that's addressed, you can used penalized regression splines, estimate with penalized LSs and select the degree of smoothing by cross validation of REML for the model
#'
#' calls the function producing the unconstrained basis and square root penalty matrices, given knot sequence xk and covariate values x
tf.XD <- function(x,xk,cmx=NULL,m=2){
## get X and D subject to constraint
nk <- length(xk)
X <- tf.X(x,xk)[,-nk] ## basis matrix
D <- diff(diag(nk),differences=m)[,-nk] ## root penalty
if(is.null(cmx)) cmx <- colMeans(X)
X <- sweep(X,2,cmx) ## subtract cmx from columns
list(X=X,D=D,cmx=cmx)
}
am.fit <- function(y,x,v,sp,k=10){
## setup bases and penalties...
xk <- seq(min(x),max(x),length=k)
xdx <- tf.XD(x,xk)
vk <- seq(min(v),max(v),length=k)
xdv <- tf.XD(v,vk)
## create augmented model matrix and response...
nD <- nrow(xdx$D)*2
sp <- sqrt(sp)
X <- cbind(c(rep(1,nrow(xdx$X)),rep(0,nD)),
rbind(xdx$X,sp[1]*xdx$D,xdx$D*0),
rbind(xdv$X,xdx$D*0,sp[2]*xdv$D))
y1 <- c(y,rep(0,nD))
## fit model...
b <- lm(y1 ~ X - 1)
## compute some useful quuantities...
n <- length(y)
trA <- sum(influence(b)$hat[1:n]) ## EDF
rsd <- y - fitted(b)[1:n] ## residuals
rss <- sum(rsd^2) ## residual SS
sig.hat <- rss/(n-trA) ## residual variance
gcv <- sig.hat*n/(n-trA) ## GCV score
Vb <- vcov(b)*sig.hat/summary(b)$sigma^2 ## coef cov matrix
## return fittted model...
list(b=coef(b),Vb=Vb,edf=trA,gcv=gcv,fitted=fitted(b)[1:n],
rsd=rsd,xk=list(xk,vk),cmx=list(xdx$cmx,xdv$cmx))
}
am.gcv <- function(lsp,y,x,v,k){
## function suitable for GCV optimization by optim
am.fit(y,x,v,exp(lsp),k)$gcv
}
## find GCV optimal smoothing parameters...
fit <- optim(c(0,0),am.gcv,y=trees$Volume,x=trees$Girth,
v=trees$Height,k=10)
sp <- exp(fit$par) ## best fit smoothing parameters
## Get fit at GCV optimal smoothing parameters
fit <- am.fit(trees$Volume,trees$Girth,trees$Height,sp,k=10)
am.plot <- function(fit,xlab,ylab){
## prodduces effect plots for simple 2 term
## additive model
start <- 2 ## where smooth coeffs start in beta
for(i in 1:2){
## sequence of values at which to predict...
x <- seq(min(fit$xk[[i]]),max(fit$xk[[i]]),length=200)
## get prediction matrix for this smooth...
Xp <- tf.XD(x,fit$xk[[i]],fit$cmx[[i]])$X
## extract coefficients and cov matrix for this smooth
stop <- start + ncol(Xp)-1; ind <- start:stop
b <- fit$b[ind];Vb <- fit$Vb[ind,ind]
## values for smooth at x...
fv <- Xp %*% b
## standard errors for smooth at x...
se <- rowSums((Xp %*% Vb)* Xp)^.5
## 2 se limits for smooth
ul <- fv + 2 * se; ll <- fv - 2 * se
## plot smooth and limits
plot(x,fv,type="l",ylim=range(c(ul,ll)),xlab=xlab[i],ylab=ylab[i])
lines(x,ul,lty=2); lines(x,ll,lty=2)
start <- stop+1
}
}
par(mfrow=c(1,3))
plot(fit$fitted,trees$Volume,xlab="fitted volume",ylab="observed volume")
am.plot(fit, xlab=c("Girth","Height"),
ylab=c("s(Girth)","s(Height)"))
#' the zero width point for height in the interval occurs because sum to zero constraint exactly  determines where the straight line must pass through zero
#'
# Generalized additive models ---------------------------------------------
#' The linear predictor now predicts some known smooth monotonic function of the expected value of the response, and the response may follow any exponential family disrttibution.
#' The model is estimated by penalized maximum likelihood versus penalized least squares but in practice it is penalized iterative least squares (PIRLS)
#'
gam.fit <- function(y,x,v,sp,k=10){
## gamma error log link 2 term gam fit...
eta <- log(y) ## initial eta
not.converged <- TRUE
old.gcv <- -100 ## don't converge immediately
while(not.converged){
mu <- exp(eta) ## current mu estimate
z <- (y - mu)/mu+eta ## pseudodata
fit <- am.fit(z,x,v,sp,k) ## penalized least squares
if(abs(fit$gcv-old.gcv)<1e-5*fit$gcv){
not.converged <- FALSE
}
old.gcv <- fit$gcv
eta <- fit$fitted ## updated eta
}
fit$fittted <- exp(fit$fitted) ## mue
fit
}
gam.gcv <- function(lsp,y,x,v,k=10){
gam.fit(y,x,v,exp(lsp),k=k)$gcv
}
## find GCV optimal smoothing parameters...
fit <- optim(c(0,0),gam.gcv,y=trees$Volume,x=trees$Girth,
v=trees$Height,k=10)
sp <- exp(fit$par) ## best fit smoothing parameters
## Get fit at GCV optimal smoothing parameters
fit <- gam.fit(trees$Volume,trees$Girth,trees$Height,sp,k=10)
par(mfrow=c(1,3))
plot(fit$fitted,trees$Volume,xlab="fitted volume",ylab="observed volume")
am.plot(fit, xlab=c("Girth","Height"),
ylab=c("s(Girth)","s(Height)"))
# Summary ------------------------------------------------------------------
#' Estimation is by penalized versions of the least squares and maximum-likelihood methods used for linear models and GLMs.
#' Indeed technically GAMs are simply GLMs estimated subject to smoothing penalties
#' Largest difficulty is estimating the degree of penalization - GCV gives a reasonable solution but so does marginal likelihood
#'
# Introducing package mgcv ------------------------------------------------
library(mgcv)
data(trees)
ct1 <- gam(Volume ~ s(Height) + s(Girth),
family=Gamma(link=log),data=trees)
ct1
plot(ct1,residuals=T)
bspline <- function(x,k,i,m=2){
# evaluate ith B-spline basis function of order m at the
# values in x, given knot locations in k
if(m==-1){ #base of the recursion
res <- as.numeric(x<k[i+1]&x>=k[i])
}else{ # construct from call to lower order basis
z0 <- (x-k[i])/(k[i+m+1]-k[i])
z1 <- (k[i+m+2]-x)/(k[i+m+2]-k[i+1])
res <- z0*bspline(x,k,i,m-1) + z1*bspline(x,k,i+1,m-1)
}
res
}
k <- 6 # example basis dimension
P <- diff(diag(k),difference=1) # sqrt of penalty matrix
S <- t(P)%*%P # penalty matrix
S
library(mgcv)
sin(1:100)
plot(sin(1:100))
plot(sin(1))
plot(sin(1:10))
?sin
sin(1:100)
sin(0:pi)
sin(100)
sin(seq(0,pi,length.out=100))
plot(sin(seq(0,pi,length.out=100)))
plot(sin(seq(0,2*pi,length.out=100)))
x = sin(seq(0,2*pi,length.out=100))
ssp <- s(x,bs="ps",k=k); ssp$mono <- 1
sm <- smoothCon(ssp,data.frame(x))[[1]]
x = sin(seq(0,2*pi,length.out=100))
ssp <- s(x,bs="ps",k=k); ssp$mono <- 1
sm <- smoothCon(ssp,data.frame(x))[[1]]
X <- sm$X; XX <- crossprod(X);sp <- .5
gamma <- rep(0,k); S <- sm$S[[1]]
for(i in 1:20){
gt <- c(gamma[1],exp(gamma[2:k]))
dg <- c(1,gt[2:k])
g <- -dg*(t(X)%*%(y-X%*%gt)) + sp*S%*%gamma
H <- dg*t(dg*XX)
gamma <- gamma - solve(H+sp*S,g)
}
for(i in 1:20){
gt <- c(gamma[1],exp(gamma[2:k]))
dg <- c(1,gt[2:k])
g <- -dg*(t(X)%*%(i-X%*%gt)) + sp*S%*%gamma
H <- dg*t(dg*XX)
gamma <- gamma - solve(H+sp*S,g)
}
x = sin(seq(0,2*pi,length.out=100)) + runif(100)
y = sin(seq(0,2*pi,length.out=100))
plot(x,y)
plot(x)
plot(y)
plot(x,y)
runif(100)
x = sin(seq(0,2*pi,length.out=100)) + runif(100)
y = sin(seq(0,2*pi,length.out=100))
ssp <- s(x,bs="ps",k=k); ssp$mono <- 1
sm <- smoothCon(ssp,data.frame(x))[[1]]
X <- sm$X; XX <- crossprod(X);sp <- .5
gamma <- rep(0,k); S <- sm$S[[1]]
for(i in 1:20){
gt <- c(gamma[1],exp(gamma[2:k]))
dg <- c(1,gt[2:k])
g <- -dg*(t(X)%*%(i-X%*%gt)) + sp*S%*%gamma
H <- dg*t(dg*XX)
gamma <- gamma - solve(H+sp*S,g)
}
x = sin(seq(0,2*pi,length.out=100)) + runif(100)
y = sin(seq(0,2*pi,length.out=100))
ssp <- s(x,bs="ps",k=k); ssp$mono <- 1
sm <- smoothCon(ssp,data.frame(x))[[1]]
X <- sm$X; XX <- crossprod(X);sp <- .5
gamma <- rep(0,k); S <- sm$S[[1]]
for(i in 1:20){
gt <- c(gamma[1],exp(gamma[2:k]))
dg <- c(1,gt[2:k])
g <- -dg*(t(X)%*%(y-X%*%gt)) + sp*S%*%gamma
H <- dg*t(dg*XX)
gamma <- gamma - solve(H+sp*S,g)
}
gamma
